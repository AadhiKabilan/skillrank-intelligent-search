# ğŸ” ArXiv Semantic Search System

A complete, minimal, production-ready full-stack intelligent search system for 24K+ research papers. Built for hackathon speed with LLM-powered semantic search, cross-document reasoning, and citation-grounded synthesis.

> **Note**: This project implements the design specifications generated by Kiro.

## ğŸ¯ Problem Overview

Traditional keyword search fails to capture semantic meaning and relationships across research papers. This system solves that by:

- **Semantic Search**: Uses embeddings instead of keywords to find conceptually related papers
- **Cross-Document Synthesis**: LLM analyzes multiple papers to provide comprehensive insights
- **Citation Grounding**: All responses include proper citations to source papers
- **Scalable Architecture**: Handles 24K+ papers with sub-3-second response times

## ğŸš€ Quick Start (30 seconds)

### Prerequisites
- Python 3.8+
- OpenAI API key (Optional - for best results. Fallback to local mode available.)
- ArXiv dataset: `arxivData.json` (Must be present in root)

### Setup & Run

```bash
# 1. Create and activate venv (Windows)
python -m venv venv
.\venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Set OpenAI API key (Optional but recommended)
set OPENAI_API_KEY=sk-your-key-here
# If skipped, system uses local sentence-transformers (No LLM synthesis)

# 4. Process papers (demo with 500 papers)
python ingest.py --limit 500

# 5. Start server
uvicorn app:app --reload

# 6. Open browser
# Navigate to http://localhost:8000
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Pipeline â”‚    â”‚   Search API     â”‚    â”‚    Frontend     â”‚
â”‚   (ingest.py)   â”‚    â”‚   (app.py)       â”‚    â”‚ (index.html)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Load JSON     â”‚â”€â”€â”€â–¶â”‚ â€¢ FastAPI Server â”‚â—€â”€â”€â”€â”‚ â€¢ Search Form   â”‚
â”‚ â€¢ Chunk Text    â”‚    â”‚ â€¢ FAISS Index    â”‚    â”‚ â€¢ Results View  â”‚
â”‚ â€¢ Generate      â”‚    â”‚ â€¢ OpenAI LLM     â”‚    â”‚ â€¢ Citations     â”‚
â”‚   Embeddings    â”‚    â”‚ â€¢ Synthesis      â”‚    â”‚ â€¢ ArXiv Links   â”‚
â”‚ â€¢ Build Index   â”‚    â”‚ â€¢ REST API       â”‚    â”‚ â€¢ Vanilla JS    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ index.faiss   â”‚    â”‚ â€¢ Vector Search  â”‚    â”‚ â€¢ No Frameworks â”‚
â”‚ â€¢ meta.json     â”‚    â”‚ â€¢ Cosine Sim     â”‚    â”‚ â€¢ Responsive    â”‚
â”‚ â€¢ Normalized    â”‚    â”‚ â€¢ Cross-Doc      â”‚    â”‚ â€¢ Real-time     â”‚
â”‚   Embeddings    â”‚    â”‚   Reasoning      â”‚    â”‚   Updates       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Implementation & Design Decisions

This implementation strictly follows the Kiro `design.md` and `requirements.md` specs.

### Design Decisions (from design.md)
1. **Simplicity**: No heavy frameworks like LangChain. Pure FastAPI + OpenAI client.
2. **Data Pipeline**: 
   - `ingest.py` effectively handles data loading, chunking (800 chars), and embedding.
   - Embeddings are normalized (L2) to ensure `IndexFlatIP` acts as Cosine Similarity.
3. **Search & Retrieval**: 
   - Uses FAISS for sub-millisecond similarity search.
   - Dedupes papers to show diverse results.
4. **LLM Integration**:
   - Uses `gpt-3.5-turbo` with `temperature=0.2` for factual consistency.
   - Prompt engineering focuses on "Cross-document reasoning" and citation format `1) Title - ID`.
5. **Fallback Strategy**:
   - If `OPENAI_API_KEY` is missing, ingestion switches to `sentence-transformers/all-MiniLM-L6-v2`.
   - Search falls back to extractive summaries if LLM is unavailable.

## âœ… Task Status

Based on `tasks.md`:

- [x] **Setup**: Project structure, requirements, gitignore.
- [x] **Ingestion (ingest.py)**:
  - [x] CLI (argparse)
  - [x] JSON Loading
  - [x] Chunking (~800 chars)
  - [x] Embeddings (OpenAI + Fallback)
  - [x] FAISS Indexing & Persistence
- [x] **Back-end (app.py)**:
  - [x] FastAPI logic
  - [x] Index Loading
  - [x] /search endpoint
  - [x] LLM Synthesis (with citations)
- [x] **Front-end**:
  - [x] Minimal HTML/CSS/JS
  - [x] Responsive layout
  - [x] Real-time search UI

## ğŸ“ Example Queries

Try these complex queries:

1. **"What are the main criticisms of BERT?"**
   - *Expectation*: A synthesized summary citing computational cost and pre-training bias.
2. **"Find alternatives to attention mechanisms"**
   - *Expectation*: Mentions of Linear Attention or State Space Models.
3. **"How does GAN training stability improve?"**
   - *Expectation*: Techniques like Wasserstein loss or spectral normalization.

## ğŸ”§ Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Language** | Python 3 | Core implementation |
| **Backend** | FastAPI | REST API server |
| **Vector Store** | FAISS (CPU) | Similarity search |
| **Embeddings** | OpenAI text-embedding-3-small | Semantic vectors |
| **LLM** | OpenAI GPT-3.5-turbo | Response synthesis |
| **Frontend** | HTML + Vanilla JS | User interface |

## âš ï¸ Note on Credits
**OpenAI API is not free.** This project will consume credits for embedding (during ingest) and chat completion (during search). 
- If you lack credits, use the `--local` flag during ingestion: `python ingest.py --limit 500 --local`.
- If you use `--local`, the system will automatically use local sentence-transformers for search as well, and fallback to extractive summaries.

---
*Completed by Antigravity based on Kiro specifications.*