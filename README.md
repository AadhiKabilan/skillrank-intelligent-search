# ğŸ” ArXiv Semantic Search System

A complete, production-ready intelligent search system for 24K+ research papers. Built for hackathon performance with **Local LLM-powered semantic search**, data-grounded synthesis, and sub-second response times.

## ğŸ¯ Problem Overview

Traditional keyword search fails to capture semantic meaning. This system solves that by:
- **Semantic Search**: Uses local vector embeddings to find conceptually related papers.
- **RAG Architecture**: Retrieval-Augmented Generation for grounded answers.
- **Local AI**: 100% Privacy-focused using Ollama (running Mistral 7B).
- **Scalable**: Indexing performed on high-performance GPUs (via Colab) and served locally.

## ğŸ“¸ Screenshots

![Search Interface](screenshots/ArXiv%20Semantic%20Search-mh.png)
_Semantic Search Interface with Complex Queries_

![Search Results](screenshots/ArXiv%20Semantic%20Search-mh%20(1).png)
_Synthesized Answer with Citations_

![Results Detail](screenshots/ArXiv%20Semantic%20Search-mh%20(2).png)
_Detailed Paper Hits and Citations_

## ğŸš€ Quick Start (30 seconds)

### Prerequisites
- Python 3.8+
- [Ollama](https://ollama.ai/) installed & running (`ollama serve`).

### Setup & Run

```bash
# 1. Start Ollama and pull the model (Mistral 7B)
ollama pull mistral:7b
ollama serve

# 2. Setup Python Environment
python -m venv venv
.\venv\Scripts\activate
pip install -r requirements.txt

# 3. Process Papers (24k Full Dataset)
# Option A: Run the `colab_ingest.py` script on Google Colab (Fastest - 5 mins).
# Option B (Local Demo):
python ingest.py --limit 1000

# 4. Start the Application
uvicorn app:app --reload

# 5. Open Browser
# Navigate to http://localhost:8000
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Ingest   â”‚      â”‚   Search API     â”‚      â”‚    Frontend     â”‚
â”‚   (Colab GPU)   â”‚      â”‚   (FastAPI)      â”‚      â”‚ (index.html)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Load JSON     â”‚â”€â”€â”€â”€â”€â–¶â”‚ â€¢ FAISS Index    â”‚â—€â”€â”€â”€â”€â”€â”‚ â€¢ Search Box    â”‚
â”‚ â€¢ Chunking      â”‚      â”‚ â€¢ Semantic       â”‚      â”‚ â€¢ Results View  â”‚
â”‚ â€¢ Embeddings    â”‚      â”‚   Search         â”‚      â”‚ â€¢ Real-time     â”‚
â”‚  (SentenceTx)   â”‚      â”‚ â€¢ Local LLM      â”‚      â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   (Ollama)       â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   Ollama / API   â”‚
                         â”‚   (Mistral 7B)   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Implementation Details

### Design Decisions
1.  **Architecture**: Split ingestion (Heavy GPU work) from Serving (Fast CPU Work).
2.  **Vector Store**: FAISS for millisecond-level similarity search.
3.  **Local LLM**: Switched to Ollama (Mistral 7B) to ensure 0% reliance on paid APIs and 100% offline capability.
4.  **Prompt Engineering**: Optimized system prompts to enforce strict citation formats `Title (ID)` required by the problem statement.

## âœ… Task Status

- [x] **Ingestion**: Scaled to 24k papers using Colab GPU acceleration.
- [x] **Vector Search**: Local FAISS index integration.
- [x] **Synthesis**: Replaced Mock/Cloud APIs with robust Local RAG (Ollama).
- [x] **Frontend**: Clean, responsive UI.

## ğŸ”§ Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Language** | Python 3 | Core logic |
| **Backend** | FastAPI | High-performance API |
| **Vector DB** | FAISS | Similarity Search |
| **LLM** | Ollama (Mistral) | Answer Synthesis |
| **Embeddings** | all-MiniLM-L6-v2 | Vector generation |

---
*Built for the SkillRank Hackathon.*